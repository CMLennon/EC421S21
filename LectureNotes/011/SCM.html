<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Structural Causal Models (SCMs)</title>
    <meta charset="utf-8" />
    <meta name="author" content="Connor Lennon" />
    <script src="SCM_files/header-attrs-2.8/header-attrs.js"></script>
    <link href="SCM_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="SCM_files/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="SCM_files/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Structural Causal Models (SCMs)
## EC 607, Set 11
### Connor Lennon
### Spring 2021

---

class: inverse, middle





&lt;style type="text/css"&gt;
@media print {
  .has-continuation {
    display: block !important;
  }
}
&lt;/style&gt;

$$
`\begin{align}
  \def\ci{\perp\mkern-10mu\perp}
\end{align}`
$$


# Prologue

---
name: schedule

# Schedule

## Last time

Rubin-Neyman Causal Model

## Today

We're covering more complex causal relationships than we did last time - these are represented by what are known as Directed Acyclic Graphs (DAGs)

## Upcoming

Instrumental Variables

---
class: inverse, middle
# SCMs

---

## Causality is Complicated

Though we'd ideally be able to run .pink[experiments] to identify causal effects, there are a number of ways experiments can fail. Just to name a few -


.pull-left[.purple[
**Pre-administration**

- Where can we 'intervene'?
- Do we have access to a truly representative sample?
- Is anyone willing to undertake the treatment/control?
- How close are we to a laboratory sample?
]]

.pull-right[.pink[
**Post-administration**

- Placebo Effect
- Reverse causality
- Defiers and Always-Takers
- Mediators (ie. Is our experiment causing this or causing something else)?
]]

---

## Causality is Complicated

This means we have to think about 

- experimental setting AND 
- what causal system we are acting on

--

Therefore it's useful to have a nice way of visualizing and understanding causal systems. Like .hi[DAGs]. 

- It's useful, however, to understand what a 'graph' actually is

---

## Causality is Complicated

We can use the tools of .slate[.b[formal graphs]] to organize our assumptions about the problem space into:

- Variables of interest
- Direct Causal Effects between them
- Potential exogeneity concerns

We can then find out...

--

- whether or not the problem we are interested in is .pink[tractable]

--

- If it is tractable, what .orange[variables] do we need to measure and control for to estimate causal effects

--

- How existing experiments might get polluted by poor specification

---

## Causality is Complicated

The graphs we are interested in for causal effects are

--

 - .pink[directed]
 
 - .slate[acyclic]
 
We restrict our problem space to these mostly because it reduces the complexity of the math.

So let's learn about .pink[graphs] ðŸ’¹

---
layout: true
#Graphs

---
class: inverse, middle
name: graphs

---

## More formally

In graph theory, a .pink.def[graph] is a collection of .purple.def[nodes] connected by .orange.def[edges].

--



&lt;img src="SCM_files/figure-html/graph-ex-undirected-1.svg" style="display: block; margin: auto;" /&gt;

--

- Nodes connected by an edge are called .def[adjacent].
--

- .def[Paths] run along adjacent nodes, .it[e.g.], `\(\text{A}-\text{B}-\text{C}\)`.
--

- The graph above is .def[undirected], since the edges don't have direction. We can give our graphs more information by defining direction for the edges

---
name: graphs-directed
## Directed

.def.purple[Directed graphs] have edges with direction.

&lt;img src="SCM_files/figure-html/graph-ex-directed-1.svg" style="display: block; margin: auto;" /&gt;

--

- .def[Directed paths] follow edges' directions, *e.g.*, `\(\text{A}\rightarrow\text{B}\rightarrow\text{C}\)`.

--
- Nodes that precede a given node in a directed path are its .def[.blue[.b[ancestors]]].

--
- The opposite: .def[.b[.pink[descendants]]] come after the node, *e.g.*, `\(\text{D}=\text{de}(\text{C})\)`.

---
name: graphs-cycles
## Cycles

If a node is its own descendant (*e.g.*, `\(\text{de}(\text{D})=\text{D}\)`), your graph has a .pink.def[cycle].




&lt;img src="SCM_files/figure-html/graph-ex-cycle-1.svg" style="display: block; margin: auto;" /&gt;

--

If your directed graph does not have any cycles, then you have a &lt;br&gt; .def.orange[directed acyclic graph] (.def.orange[DAG]).


---
layout: true
# DAGs

---
name: different
##  What's a DAG?

.note[DAG] stands for .b[directed acyclic graph].

--

Formally, any graph is a collection of .orange[nodes] and edges defined by their connections.

--

.note[DAGs] are special graphs that help us understand causality in much the same way as microeconomic models help us understand markets

---
##  What's a DAG?

.note[DAG] stands for .b[directed acyclic graph].

More helpful...

A .note[DAG] is a graph that features a collection of .orange[nodes] and .blue[**directed** edges]

--

The graph illustrates and differentiates the causal associations and non-causal associations within a network of "random" variables.

---
name: different3
##  Why a DAG?

Following a fairly simple set of rules, DAGs allow researchers to visualize complex systems.

It separates -

 - .blue[causal] 'associations'
 - .pink[noncausal] 'associations' 
 
 in any assumption space without cycles

---
name: dag-ex
layout: false
class: clear

.ex[Example] Omitted-variable bias in a DAG 



&lt;img src="SCM_files/figure-html/dag-ex-ovb-1.svg" style="display: block; margin: auto;" /&gt;

A pretty standard DAG.

---
class: clear

.ex[Example] Omitted-variable bias in a DAG 

&lt;img src="SCM_files/figure-html/dag-ex-ovb-nodes-1.svg" style="display: block; margin: auto;" /&gt;

.b.orange[Nodes] are random variables.

---
class: clear

.ex[Example] Omitted-variable bias in a DAG 

&lt;img src="SCM_files/figure-html/dag-ex-ovb-edges-1.svg" style="display: block; margin: auto;" /&gt;

.b.blue[Edges] depict causal links. Causality flows in the direction of the .b.purple[arrows].

--

- Connections are drawn between nodes that .hi[directly] cause one another
- Direction matters (for causality).
- Non-connections also (sometimes) matter! .grey-light[(More on this topic soon.)]

---
class: clear

.ex[Example] Omitted-variable bias in a DAG 

&lt;img src="SCM_files/figure-html/dag-ex-ovb-2-1.svg" style="display: block; margin: auto;" /&gt;

Here we can see that .b.slate[Y] is affected by both .b.slate[D] and .b.slate[W].

.b.slate[W] also affects .b.slate[D].

--

.qa[Q] How does this graph exhibit OVB?

---
class: clear

.ex[Example] Omitted-variable bias in a DAG 

&lt;img src="SCM_files/figure-html/dag-ex-ovb-3-1.svg" style="display: block; margin: auto;" /&gt;

There are two pathways from .b.slate[D] to .b.slate[Y].

--

.slate[1\.] The path from .b.slate[D] to .b.slate[Y] `\(\color{#e64173}{\left(\text{D}\rightarrow\text{Y}\right)}\)` is our casual relationship of interest. 

---
class: clear

.ex[Example] Omitted-variable bias in a DAG 

&lt;img src="SCM_files/figure-html/dag-ex-ovb-4-1.svg" style="display: block; margin: auto;" /&gt;

There are two pathways from .b.slate[D] to .b.slate[Y].

.slate[1\.] The path from .b.slate[D] to .b.slate[Y] `\(\color{#314f4f}{\left(\text{D}\rightarrow\text{Y}\right)}\)` is our casual relationship of interest. 
&lt;br&gt;
.slate[2\.] The path `\(\color{#e64173}{\left(\text{Y}\leftarrow\text{W}\rightarrow\text{D}\right)}\)` creates a .orange[non-causal association] btn .b.slate[D] and .b.slate[Y].

---
class: clear

.ex[Example] Omitted-variable bias in a DAG 

&lt;img src="SCM_files/figure-html/dag-ex-ovb-6-1.svg" style="display: block; margin: auto;" /&gt;

There are two pathways from .b.slate[D] to .b.slate[Y].

.slate[1\.] The path from .b.slate[D] to .b.slate[Y] `\(\color{#314f4f}{\left(\text{D}\rightarrow\text{Y}\right)}\)` is our casual relationship of interest. 
&lt;br&gt;
.slate[2\.] The path `\(\color{#314f4f}{\left(\text{Y}\leftarrow\text{W}\rightarrow\text{D}\right)}\)` creates a .orange[non-causal association] btn .b.slate[D] and .b.slate[Y].

To shut down this pathway creating a non-causal association, we must .b.grey-light[condition on .b[W]]. Sound familiar?




---
layout: true
# DAGs

---
class: inverse, middle

---
name: dag-origins
## The origin story

Many developments in .it[causal graphical models] came from work in probabilistic graphical modelsâ€”especially Bayesian networks.

--

Using bayes rule, we can decompose any joint probability distribution into a product of conditional distributions 

--

I won't require you guys to learn the formal math behind Dags, though it is totally doable.

If you go to the end of the lecture, there will be an extensive section covering the theory/proofs.

---
name: local-markov
## Thinking locally

DAGs help us think through how we can break down complex causal systems of many variables - simplifying `\(P(x_k | x_{k-1},x_{k-2},\ldots,x_1)\)`.

--

&lt;img src="SCM_files/figure-html/graph-prob-1.svg" style="display: block; margin: auto;" /&gt;

Given a prob. dist. and a DAG, can we assume some independencies?
--
&lt;br&gt; Given `\(\color{#6A5ACD}{\text{C}}\)`, is it reasonable to assume `\(\color{#6A5ACD}{\text{D}}\)` is independent of `\(\color{#6A5ACD}{\text{A}}\)` and `\(\color{#6A5ACD}{\text{B}}\)`?


---
## Local Markov

This intuitive approach *is* the .def.purple[Local Markov Assumption]
&gt; Given its parents in the DAG, a node `\(X\)` is independent of all of its non-descendants.
--

.more-left[
.ex[Ex.] Consider the DAG to the right:

With the Local Markov Assumption,
&lt;br&gt;
`\(P(\text{D}|\text{A},\text{B},\text{C})\)` simplifies to `\(P(\text{D}|\text{C})\)`.

Conditional on its parent `\((\text{C})\)`,
&lt;br&gt;
`\(\text{D}\)` is independent of `\(\text{A}\)` and `\(\text{B}\)`.
]

.less-right[
&lt;img src="SCM_files/figure-html/graph-prob-2-1.svg" style="display: block; margin: auto;" /&gt;
]

---
## Independence

What have we learned so far? .grey-vlight[(Why should you care about this stuff?)]

Local Markov tells us abound .attn[independencies] within a probability distribution implied by the given DAG.

You're now able to say something about which variables are .pink.it[independent].

--

.b[There's more:] Great start, but there's more to life than independence.
&lt;br&gt;We also want to say something about .purple.it[dependence].

---
name: dags-dependence
## Dependence

We need to strengthen our Local Markov assumption to be able to interpret adjacent nodes as dependent. .grey-vlight[(*I.e.*, add it to our small set of assumptions.)]

--

The .def.purple[Minimality Assumption].pink.super[â€ ]
&gt; 1. .def.purple[Local Markov] Given its parents in the DAG, a node `\(X\)` is independent of all of its non-descendants.
&gt; 2. .it.grey-light[(NEW)] Adjacent nodes in the DAG are dependent.
.footnote[
.pink[â€ ] The name .grey-light.def[minimality] refers to the minimal set of independencies for `\(P\)` and `\(G\)`â€”we cannot remove any more edges from the graph (while staying Markov compatible with `\(G\)`).]

--

With the minimality assumption, we can learn both .pink[dependence] and .orange[independence] from connections (or non-connections) in a DAG. 

---
name: dags-causlity
## Causality

We need one last assumption move DAGs from .it[statistical] to .it[causal] models.

--

.def.purple[Strict Causal Edges Assumption]
&gt; Every parent is a direct cause of each of its children.
--

For `\(Y\)`, the set of .it[direct causes] is the set of variables to which `\(Y\)` responds.

--

This assumption actually strengthens the second part of .note[Minimality]:
&gt; 2\. Adjacent nodes in the DAG are dependent.
---
## Assumptions

Thus, we only need two assumptions to turn DAGs into causal models:

1. .def.purple[Local Markov] Given its parents in the DAG, a node `\(X\)` is independent of all of its non-descendants.

1. .def.purple[Strict Causal Edges] Every parent is a direct cause of each of its children.

--

Not bad, right?

---
## Flows

[Brady Neal](https://bradyneal.com) emphasizes the .note[flow(s) of association] and .note[causation] in DAGs,
&lt;br&gt;and I find it to be a super helpful way to think about these models.

.def.purple[Flow of association] refers to whether two nodes are associated (statistically dependent) or not (statistically independent).

We will be interested in unconditional and conditional associations.

---
name: building-blocks
## Building blocks

We will run through a few simple .it[building blocks] (DAGs) that make up more complex DAGs.

For each simple DAG, we want to ask a few questions:

1. Which nodes are unconditionally or conditionally .b.pink[independent]?.super.pink[â€ ]

1. Which nodes are .b.orange[dependent]?

1. What is the .b.purple[intuition]?

.footnote[
.pink[â€ ] To prove `\(\text{A}\)` and `\(\text{B}\)` are conditionally independent, we can show `\(P(\text{A},\text{B}|\text{C})\)` factorizes as `\(P(\text{A}|\text{C})P(\text{B}|\text{C})\)`.]

---
layout: true
class: clear

---
.note[Building block 1:] .b.slate[Two unconnected nodes]

&lt;img src="SCM_files/figure-html/bb1-plot-1.svg" style="display: block; margin: auto;" /&gt;

--

.b.purple[Intuition:]
--
 `\(\text{A}\)` and `\(\text{B}\)` appear independentâ€”no link between the nodes.

--

.b.pink[Proof:]
--
 By [Bayesian network factorization](#factorization),
$$
`\begin{align}
  P(\text{A},\text{B}) = P(\text{A}) P(\text{B})
\end{align}`
$$
(since neither node has parents). `\(\checkmark\)`

---
.note[Building block 2:] .b.slate[Two connected nodes]



&lt;img src="SCM_files/figure-html/bb2-plot-1.svg" style="display: block; margin: auto;" /&gt;

--

.b.purple[Intuition:]
--
 `\(\text{A}\)` "is a cause of" `\(\text{B}\)`: there is clear (causal) dependence.
--
.b.pink[Proof:]
--

By the [Strict Causal Edges Assumption](#dags-causlity), every parent (here, `\(\text{A}\)`) is a direct cause of each of its children `\(\left(\text{B}\right)\)`. `\(\checkmark\)`
---
name: blocks-chains
.note[Building block 3:] .b.slate[Chains]


&lt;img src="SCM_files/figure-html/bb3-plot-1.svg" style="display: block; margin: auto;" /&gt;

--

.b.purple[Intuition:] We already showed two connected nodes are dependent:
- `\(\text{A}\)` and `\(\text{B}\)` are dependent.
- `\(\text{B}\)` and `\(\text{C}\)` are dependent.

The question is whether `\(\text{A}\)` and `\(\text{C}\)` are dependent:
&lt;br&gt;Does association flow from `\(\text{A}\)` to `\(\text{C}\)` through `\(\text{B}\)`?

---
count: false
.note[Building block 3:] .b.slate[Chains]

&lt;img src="SCM_files/figure-html/bb3-plot-2-1.svg" style="display: block; margin: auto;" /&gt;

.b.purple[Intuition:] We already showed two connected nodes are dependent:
- `\(\text{A}\)` and `\(\text{B}\)` are dependent.
- `\(\text{B}\)` and `\(\text{C}\)` are dependent.

The question is whether `\(\text{A}\)` and `\(\text{C}\)` are dependent:
&lt;br&gt;Does association flow from `\(\text{A}\)` to `\(\text{C}\)` through `\(\text{B}\)`? 

The answer .it[generally].super.pink[â€ ] is .orange["yes"]: changes in `\(\text{A}\)` typically cause changes in `\(\text{C}\)`. 

.footnote[
.pink[â€ ] Section 2.2 of [Pearl, Glymour, and Jewell](http://bayes.cs.ucla.edu/PRIMER/) provides a "pathological" example of "intransitive dependence". It's basically when `\(\text{A}\)` induces variation in `\(\text{B}\)` that is not relevant to `\(\text{C}\)`' outcome.]

---
.note[Building block 3:] .b.slate[Chains]

&lt;img src="SCM_files/figure-html/bb3-plot-3-1.svg" style="display: block; margin: auto;" /&gt;

.b.pink[Proof:] Here's the unsatisfying part. 

Without more assumptions, we can't *prove* this association of `\(\text{A}\)` and `\(\text{C}\)`.

We'll think of this as a potential (even likely) association.

---
.note[Building block 3:] .b.slate[Chains with conditions]

&lt;img src="SCM_files/figure-html/bb3-plot-4-1.svg" style="display: block; margin: auto;" /&gt;

.qa[Q] How does conditioning on `\(\text{B}\)` affect the association between `\(\text{A}\)` and `\(\text{C}\)`?

.b.purple[Intuition:] 

1. `\(\text{A}\)` affects `\(\text{C}\)` by changing `\(\text{B}\)`. 
2. When we hold `\(\text{B}\)` constant, `\(\text{A}\)` cannot "reach" `\(\text{C}\)`.

We've .def.purple[blocked] the path of association between `\(\text{A}\)` and `\(\text{C}\)`.

Conditioning blocks the flow of association .b[in chains]. ("Good" control!)

---
.note[Building block 3:] .b.slate[Chains]

&lt;img src="SCM_files/figure-html/bb3-plot-7-1.svg" style="display: block; margin: auto;" /&gt;

.note[Note] This .orange[association of] `\(\color{#FFA500}{\text{A}}\)` .orange[and] `\(\color{#FFA500}{\text{C}}\)` is not directional. (It is symmetric.)

On the other hand, causation .b[is] directional (and asymmetric).

As you've been warned for years: Associations are not necessarily causal.

---
name: blocks-forks

.note[Building block 4:] .b.slate[Forks]



&lt;img src="SCM_files/figure-html/bb4-plot-1-1.svg" style="display: block; margin: auto;" /&gt;

.def.purple[Forks] are another very common structure in DAGs: `\(\text{A}\leftarrow \text{B} \rightarrow \text{C}\)`.

---
.note[Building block 4:] .b.slate[Forks]

&lt;img src="SCM_files/figure-html/bb4-plot-2-1.svg" style="display: block; margin: auto;" /&gt;

`\(\text{A}\)` and `\(\text{C}\)` are *usually* .orange[associated] in forks. .grey-light[(As with chains.)]

This chain of association follows the path `\(\text{A}\leftarrow \text{B} \rightarrow \text{C}\)`.

--

.b.purple[Intuition:] 
--
 `\(\text{B}\)` induces changes in `\(\text{A}\)` and `\(\text{B}\)`. An observer will see `\(\text{A}\)` change when `\(\text{C}\)` also changesâ€”they are associated due to their common cause.

---
.note[Building block 4:] .b.slate[Forks]



&lt;img src="SCM_files/figure-html/bb4-plot-ovb-1.svg" style="display: block; margin: auto;" /&gt;

Another way to think about forks: 

OVB when a treatment `\(\text{D}\)` does not affect the outcome `\(\text{Y}\)`.

Without controlling for `\(\text{W}\)`, `\(\text{Y}\)` and `\(\text{D}\)` are (usually) .orange[non-causally associated].

---
.note[Building block 4:] .b.slate[Forks]

&lt;img src="SCM_files/figure-html/bb4-plot-3-1.svg" style="display: block; margin: auto;" /&gt;

`\(\text{A}\)` and `\(\text{C}\)` are *usually* .orange[associated] in forks. .grey-light[(As with chains.)]

This chain of association follows the path `\(\text{A}\leftarrow \text{B} \rightarrow \text{C}\)`.

.b.pink[Proof:]
--
 Same problem as chains: We can't show `\(\text{A}\)` and `\(\text{C}\)` are independent, so we assume they're likely (potentially?) dependent.

---
.note[Building block 4:] .b.slate[Blocked forks]

&lt;img src="SCM_files/figure-html/bb4-plot-4-1.svg" style="display: block; margin: auto;" /&gt;

Conditioning on `\(\text{B}\)` makes `\(\text{A}\)` and `\(\text{C}\)`
--
 independent. .grey-light[(As with chains.)]

.b.purple[Intuition:]
--
 `\(\text{A}\)` and `\(\text{C}\)` are only associated due to their common cause `\(\text{B}\)`.

When we shutdown (hold constant) this common cause `\((\text{B})\)`, 
&lt;br&gt;there is way for `\(\text{A}\)` and `\(\text{C}\)` to associate.

--

.note[Also:] Think about Local Markov. Or think about OVB.


---
.note[Building block 4:] .b.slate[Forks]

&lt;img src="SCM_files/figure-html/bb4-plot-6-1.svg" style="display: block; margin: auto;" /&gt;

Two more items to emphasize:

1. .b.orange[Association] need not follow paths' directions, *e.g.*, `\(\text{A}\leftarrow \text{B} \rightarrow \text{C}\)`.

2. .b.pink[Causation] follows directed paths.

---
name: blocks-colliders
.note[Building block 5:] .b.slate[Immoralities]



&lt;img src="SCM_files/figure-html/bb5-plot-1-1.svg" style="display: block; margin: auto;" /&gt;

 An .def.purple[immorality] occurs when two nodes share a child without being otherwise connected..super.pink[â€ ] `\(\text{A} \rightarrow \text{B} \leftarrow \text{C}\)`

.footnote[.pink[â€ ] Yes, this is a stupid term, and I hate it. I way prefer just using 'collider']

--

The child (here: `\(\text{B}\)`) at the center of this immorality is called a .def.purple[collider].

--

.note[Notice:] An immorality is a fork with reversed directions of the edges.

---
.note[Building block 5:] .b.slate[Immoralities]

&lt;img src="SCM_files/figure-html/bb5-plot-2-1.svg" style="display: block; margin: auto;" /&gt;

.qa[Q] Are `\(\text{A}\)` and `\(\text{C}\)` independent?

---
count: false
.note[Building block 5:] .b.slate[Immoralities]

&lt;img src="SCM_files/figure-html/bb5-plot-3-1.svg" style="display: block; margin: auto;" /&gt;

.qa[Q] Are `\(\text{A}\)` and `\(\text{C}\)` independent?
&lt;br&gt;
.qa[A] Yes. `\(\text{A} \ci \text{C}\)`.

--

.b.purple[Intuition:] Causal effects flow from `\(\text{A}\)` and `\(\text{C}\)` and stop there.

- Neither `\(\text{A}\)` nor `\(\text{C}\)` is a descendant of the other.
- `\(\text{A}\)` and `\(\text{C}\)` do not share any common causes.

---

.note[Building block 5:] .b.slate[Immoralities with conditions]

&lt;img src="SCM_files/figure-html/bb5-plot-5-1.svg" style="display: block; margin: auto;" /&gt;

.qa[Q] What happens when we condition on `\(\text{B}\)`?

---
count: false
.note[Building block 5:] .b.slate[Immoralities with conditions]

&lt;img src="SCM_files/figure-html/bb5-plot-6-1.svg" style="display: block; margin: auto;" /&gt;

.qa[Q] What happens when we condition on `\(\text{B}\)`?
&lt;br&gt;
.qa[A] We .def.orange[unblock] (or .def.orange[open]) the previously blocked (closed) path.

While `\(\text{A}\)` and `\(\text{C}\)` are independent, they are .orange[conditionally dependent].

--

.attn[Important:] When you condition on a collider, you open up the path.

---
.note[Building block 5:] .b.slate[Immoralities with conditions]

&lt;img src="SCM_files/figure-html/bb5-plot-7-1.svg" style="display: block; margin: auto;" /&gt;

.b.purple[Intuition:] `\(\text{B}\)` is a combination of `\(\text{A}\)` and `\(\text{C}\)`. 

Conditioning on a value of `\(\text{B}\)` jointly constrains `\(\text{A}\)` and `\(\text{C}\)`â€”they can no longer move independently.

--

.ex[Example:] Let `\(\text{A}\)` take on `\(\{0,1\}\)` and `\(\text{C}\)` take on `\(\{0,1\}\)` (independently).

Conditional on `\(\text{B}=1\)`, `\(\text{A}\)` and `\(\text{C}\)` are perfectly negatively correlated.

---
.note[Building block 5:] .b.slate[Immoralities with conditions]

&lt;img src="SCM_files/figure-html/bb5-plot-8-1.svg" style="display: block; margin: auto;" /&gt;

In *MHE* vocabulary: The collider `\(\text{X}\)` is a *bad control*. 

`\(\text{X}\)` is affected by both your treatment `\(\text{D}\)` and outcome `\(\text{Y}\)`.

--

.note[The result:] A spurious relationship between `\(\text{Y}\)` and `\(\text{D}\)`
&lt;br&gt;Remember: they're actually (unconditionally) independent.

--

This spurious relationship is often called .def.purple[collider bias].

---

.ex[Example] Obesity, Mortality Factors and Cardiovascular Disease.

Define `\(\text{O}\)` as .it[obesity],
--
 `\(\text{A}\)` as .it[age of patient],
--
 and `\(\text{M}\)` as .it[mortality].

--

Suppose for the moment obesity and age
1. are .b[independent of each other] 
2. each .b[cause cardiovascular disease] and cvd `\(\uparrow\text{M}\)`

--



&lt;img src="SCM_files/figure-html/dagdiag_collex-1.svg" style="display: block; margin: auto;" /&gt;

The implied DAG.

---

.ex[Example] Obesity, Cardiovascular Disease and Mortality.

Define `\(\text{O}\)` as .it[obesity], `\(\text{A}\)` as Age, `\(\text{C}\)` as .it[Cardiovascular Disease], and `\(\text{M}\)` as .it[Mortality].

&lt;img src="SCM_files/figure-html/ex-collider-bias-1b-1.svg" style="display: block; margin: auto;" /&gt;

If we .b.pink[do not condition on Cardiovascular Disease], `\(\text{S}\rightarrow \text{H} \leftarrow \text{A}\)` is .b.grey[blocked], but `\(\text{O}\rightarrow \text{C} \rightarrow \text{M}\)` is not

---

.ex[Example] Obesity, Cardiovascular Disease and Mortality.

Define `\(\text{O}\)` as .it[obesity], `\(\text{A}\)` as Age, `\(\text{C}\)` as .it[Cardiovascular Disease], and `\(\text{M}\)` as .it[Mortality].

&lt;img src="SCM_files/figure-html/ex-collider-bias-2-1.svg" style="display: block; margin: auto;" /&gt;

Our data .b.grey[conditions on cardiovascular disease], which .b.orange[opens] `\(\text{O}\rightarrow \text{C} \leftarrow \text{A}\)`.

---
class: clear, middle

You can also see this example graphically...

---
.ex[Example] Obesity, Cardiovascular Disease and Mortality.



--

&lt;img src="SCM_files/figure-html/ex-collider-plot1-1.svg" style="display: block; margin: auto;" /&gt;

--

.note[Without conditioning:] Positive relationship between obesity  and mortality.

---
.ex[Example] Obesity, Cardiovascular Disease and Mortality.

&lt;img src="SCM_files/figure-html/ex-collider-plot2-1.svg" style="display: block; margin: auto;" /&gt;

.note[Recall:] Our sample excludes individuals without cv-disease.

---
.ex[Example] Obesity, Cardiovascular Disease and Mortality.

&lt;img src="SCM_files/figure-html/ex-collider-plot3-1.svg" style="display: block; margin: auto;" /&gt;

.note[Conditioning on] `\(\text{C}\)`.note[:] Obesity now only increases mortality through Age and there are disproportionately large numbers of young obese patients with cvd.

---

Across the general population...

--


```r
lm(data = cb_dt, M ~ O) %&gt;% tidy() %&gt;% filter(term == 'O')
```

<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-2">
<col><col><col><col><col><tr>
<th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">term</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">estimate</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">std.error</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">statistic</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">p.value</th></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">O</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.64</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.106</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">6.04</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">2.74e-08</td></tr>
</table>

--

Only on patients with CVD...


```r
lm(data = cb_dt %&gt;% filter(C == 1), M ~ O) %&gt;% tidy()%&gt;% filter(term == 'O')
```

<table class="huxtable" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; ; margin-left: auto; margin-right: auto;  " id="tab:unnamed-chunk-3">
<col><col><col><col><col><tr>
<th style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">term</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">estimate</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">std.error</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">statistic</th><th style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: bold;">p.value</th></tr>
<tr>
<td style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0.4pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">O</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">-0.0792</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.0339</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">-2.34</td><td style="vertical-align: top; text-align: right; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0.4pt 0.4pt 0pt;    padding: 6pt 6pt 6pt 6pt; background-color: rgb(242, 242, 242); font-weight: normal;">0.0229</td></tr>
</table>


---
class: clear, middle

I like this example because it reminds us that .b.it[conditioning] occurs both .b[explicitly] (*e.g.*, "controlling for") and .b[implicitly] (*e.g.*, sample inclusion/exclusion).

This example of collider bias in hospitalization data comes from V. Viallon &amp; M. Dufournet's 2016 paper .it[[Can collider bias fully explain the obesity paradox?](https://arxiv.org/abs/1612.06547)]. 

.note[More generally:] You'll hear this called .def.slate[selection bias] or .def.slate[Berkson's paradox].



---
layout: false
class: inverse, middle
name: ex
# Examples

---
layout: true
class: clear

---
.ex[Example 1:] .b[OVB]

&lt;br&gt;

.to-middle[
&lt;img src="SCM_files/figure-html/ex-1-1.svg" style="display: block; margin: auto;" /&gt;
]

.qa[Q] OVB using DAG fundamentals: When can we isolate causal effects?

---
.ex[Example 2:] .b[Mediation]

Here `\(\text{M}\)` is a .def.purple[mediator]: it .def.purple[mediates] the effect of `\(\text{D}\)` on `\(\text{Y}\)`.



&lt;img src="SCM_files/figure-html/ex-2-fig-1.svg" style="display: block; margin: auto;" /&gt;

.qa[Q.sub[1]] What do we need to condition on to get the effect of `\(\text{D}\)` on `\(\text{Y}\)`?
&lt;br&gt;
.qa[Q.sub[2]] What happens if we condition on `\(\text{W}\)` and `\(\text{M}\)`?

---
.ex[Example 3:] .b[Partial mediation]

&lt;br&gt;

&lt;img src="SCM_files/figure-html/ex-3-fig-1.svg" style="display: block; margin: auto;" /&gt;

.qa[Q.sub[1]] What do we need to condition on to get the effect of `\(\text{D}\)` on `\(\text{Y}\)`?
&lt;br&gt;
.qa[Q.sub[2]] What happens if we condition on `\(\text{W}\)` and `\(\text{M}\)`?

---
.ex[Example 4:] .b[Non-mediator descendants] 

&lt;br&gt;



&lt;img src="SCM_files/figure-html/ex-4-fig-1.svg" style="display: block; margin: auto;" /&gt;

.qa[Q.sub[1]] What do we need to condition on to get the effect of `\(\text{D}\)` on `\(\text{Y}\)`?
&lt;br&gt;
.qa[Q.sub[2]] What happens if we condition on `\(\text{W}\)` and/or `\(\text{Z}\)`?


---
.ex[Example 5] .b[M-Bias]

Notice that `\(\text{C}\)` here is .it[not] a result of treatment (could be "pre-treatment").



&lt;img src="SCM_files/figure-html/ex-5-fig-1.svg" style="display: block; margin: auto;" /&gt;

.qa[Q.sub[1]] What do we need to condition on to get the effect of `\(\text{D}\)` on `\(\text{Y}\)`?
&lt;br&gt;
.qa[Q.sub[2]] What happens if we condition on `\(\text{W}\)` and/or `\(\text{Z}\)`?

---
class: middle

.b[One more note:] 

DAGs are often drawn without "noise variables" (disturbances).

But they still existâ€”they're just "outside of the model."

---
layout: false
class: inverse, middle
name: exp
# Experiments in SCM

---
#Experiments

So - how do we think about a randomized experiment with a SCM?

- Recall, an experiment induces random noise in a variable that is unrelated to other causal factors

We can think about an experiment .pink[deleting] the edges out of the experiment. Let's return to our fertilizer example.

--

If we induce random noise in fertilizer - we know that no effect that .purple[causes] fertilizer placement is related to the random placement

--

But if our experiment works, then things .orange[caused] by fertilizer should still be impacted

---
#Experiments

.note[Fertilizer on Yield: pre-experiment]



&lt;img src="SCM_files/figure-html/ex-2-fig-prexp-1.svg" style="display: block; margin: auto;" /&gt;
---
#Experiments

.note[Fertilizer on Yield: post-experiment]



&lt;img src="SCM_files/figure-html/ex-2-fig-pstexp-1.svg" style="display: block; margin: auto;" /&gt;
---
layout: false
name: limits
# DAGs
## Limitations

So what can't DAGs do (well)?

--

- .def[Simultaneity:] Defined causality as unidirectional and prohibited cycles.

--

- .def[Dynamics:] You can sort of allow a variable to affect itself... `\(\text{Y}_{t=1}\rightarrow\text{Y}_{t=2}\)`.

--

- .def[Uncertainty:] DAGs are most useful when you can correctly draw them.

--

- .def[Make friends:] There's *a lot* of (angry/uncharitable) fighting about DAGs: 
$$
`\begin{align}
  \text{Philosophy}\rightarrow \text{DAGs/Epidemiology} \leftarrow \text{Economics}
\end{align}`
$$

--

- .def[Functional Form:] All of the existing tools are functionally indifferent - but functional forms are useful (as we've seen.)

---
name: sources
layout: false

# Sources

## Thanks

These notes rely heavily upon Brady Neal's [*Introduction to Causal Inference*](https://bradyneal.com/causal-inference-course). 

I also borrow from Scott Cunningham's [*Causal Inference: The Mixtape*](https://www.scunning.com/mixtape.html).

I found the [Sackett (1978)](https://catalogofbias.org/biases/collider-bias/) example on the ["Catalog of Bias"](https://catalogofbias.org/biases/collider-bias/) website.

---
exclude: false
# Table of contents

.less-left[
.small[
#### Admin
- [Today and upcoming](#admin)

#### Other
- [Sources](#sources)

#### DAGs
- [What's a DAG?](#different)
- [Example](#dag-ex)
- [Graphs](#graphs)
  - [Definition/undirected](#graphs)
  - [Directed](#graphs-directed)
  - [Cycles](#graphs-cycles)

]
]
.more-right[
.small[
#### DAGs continued
- [Origins](#dag-origins)
  - [Local Markov](#local-markov)
  - [Bayesian net. factorization](#dags-factor)
  - [Dependence](#dags-dependence)
  - [Causality](#dags-causlity)
- [DAG building blocks](#building-blocks)
  - [Chains](#blocks-chains)
  - [Forks](#blocks-forks)
  - [Immoralities/colliders](#blocks-colliders)
- [d-separation](#d-sep)
- [Examples](#ex)
- [Limitations](#limits)
]
]

---

## Math and Probability

- Let's quickly review some probability concepts (we'll need them for this section)

---
# Theory 

## Brief Probability Review

We can decompose any joint probability into a conditional probability and it's product

.slate[a.)] `\(\mathop{P}(A \cap B) = \mathop{P}(A|B)\mathop{P}(B)\)`

--

We can decompose any conditional probability into the ratio of the joint probability and the probability of it's ratio like so

.slate[b.)] `\(\mathop{P}(A|B) = \frac{\mathop{P}(A \cap B)}{\mathop{P}(B)}\)`

--

We can substitute  .slate[a] into .slate[b] and get .pink[bayes rule] - ditching the intersection notation

`\(\mathop{P}(B|A) = \frac{\mathop{P}(A|B)\mathop{P}(B)}{\mathop{P}(A)}\)`


---
# Theory

## Product Rule

The important piece of this however is the .pink[product rule.]

By using our definitions from before, we can decompose larger and larger joint probability distributions, like so -

$$
`\begin{align}
  &amp;\color{#FFA500}{2} &amp;P(x_1,x_2) &amp;= P(x_1) P(x_2|x_1)
  \\[0.2em]
  &amp;\color{#FFA500}{3} &amp;P(x_1,x_2,x_3) &amp;= P(x_1) P(x_2,x_3|x_1) = P(x_1) P(x_2|x_1) P(x_3|x_2,x_1)
  \\[0.2em]
  &amp;\color{#FFA500}{\vdots} 
  \\[0.2em]
  &amp;\color{#FFA500}{n} &amp;P(x_1,x_2,\dots,x_n) &amp;= P(x_1)\prod_{i=2}^{n} P(x_i|x_{i-1},\ldots,x_1)
\end{align}`
$$

--

This final product can include *a lot* of terms.
&lt;br&gt;.ex[E.g.,] even when `\(x_i\)` are binary, `\(P(x_4 | x_3,x_2,x_1)\)` requires `\(2^3=8\)` parameters.

---
#Theory

## Local Markov

This intuitive approach *is* the .def.purple[Local Markov Assumption]
&gt; Given its parents in the DAG, a node `\(X\)` is independent of all of its non-descendants.
--

.more-left[
.ex[Ex.] Consider the DAG to the right:

With the Local Markov Assumption,
&lt;br&gt;
`\(P(\text{D}|\text{A},\text{B},\text{C})\)` simplifies to `\(P(\text{D}|\text{C})\)`.

Conditional on its parent `\((\text{C})\)`,
&lt;br&gt;
`\(\text{D}\)` is independent of `\(\text{A}\)` and `\(\text{B}\)`.
]

.less-right[
&lt;img src="SCM_files/figure-html/graph-prob-2b-1.svg" style="display: block; margin: auto;" /&gt;
]

---
name: dags-factor
## Local Markov and factorization

The Local Markov Assumption is equiv. to .def.purple[Bayesian Network Factorization]
&gt; For prob. dist. `\(P\)` and DAG `\(G\)`, `\(P\)` factorizes according to `\(G\)` if 
$$
`\begin{align}
  P(x_1,\ldots,x_n) = \prod_{i} P(x_i|\text{pa}_i)
\end{align}`
$$
where `\(\text{pa}_i\)` refers to `\(x_i\)`'s parents in `\(G\)`.
Bayesian network factorization is also called *the chain rule for Bayesian networks* and *Markov compatibility*.

---
# Theory
## Factorize!

You can now (more easily) factorize a causal system!

- Let's think about this graph...

.left-col[
&lt;img src="SCM_files/figure-html/graph-factorizeb-1.svg" style="display: block; margin: auto;" /&gt;
]

---
# Theory
## Factorize!

You can now (more easily) factorize a causal system!

- Let's think about this graph...

.right-col[
.b.slate[Factorization via B.N. chain rule]

$$
`\begin{align}
  &amp;P(\text{A},\text{B},\text{C},\text{D}) 
  \\[0.4em]
  &amp;\quad = \prod_{i} P(x_i|\text{pa}_i)
  \\[0.4em]
  &amp;\quad = P(\text{A}) P(\text{B}|\text{A}) P(\text{C}|\text{A},\text{B}) P(\text{D}|\text{C})
\end{align}`
$$
]

---
layout: false
# Theory
## Blocked paths

Let's formally define a blocked path (blocking is important).

--

A path between `\(\text{X}\)` and `\(\text{Y}\)` is .def.purple[blocked] by conditioning on variables `\(\text{Z}\)` (potentially no variables) if either of the following statements is true:

1. On the path `\(\text{X} \rightarrow \dots \text{Y}\)`, there is either a .b.pink[chain] or a .b.orange[fork], and we condition on some element `\(\text{w}\)` in that chain, ie `\(\text{w}\)` is in `\(\text{Z}\)`.

2. On the path, there is a .b.purple[collider] `\(\left(\dots \rightarrow \text{W} \leftarrow \dots\right)\)`, and we .it[do not] condition on it or any of its .b.orange[descendants] `\(\left(\text{de}(W)\right)\)`.

--

Association flows along unblocked paths.

---
name: d-sep-theory
# Theory
## d-separation and d-connected(-ness)

Finally, we'll define whether nodes are .note[separated] or .note[connected] in DAGs.

--

.b.purple[Separation:] Nodes `\(\text{X}\)` and `\(\text{Y}\)` are .def.purple[d-separated] by a set of nodes `\(\text{Z}\)` if .purple[all paths] between `\(\text{X}\)` and `\(\text{Y}\)` .purple[are blocked] by `\(\text{Z}\)`.

--

.b.pink[Connection:] If there is at least .pink[one path] between `\(\text{X}\)` and `\(\text{Y}\)` that is .pink[unblocked], then `\(\text{X}\)` and `\(\text{Y}\)` are .def.pink[d-connected].

---
# Theory
## d-separation and causality

.purple[d-separation] tells us that two nodes are .purple[not associated].

--

To measure the .pink[causal effect] of `\(\text{X}\)` on `\(\text{Y}\)`:
&lt;br&gt;We must eliminate .pink[non-causal association].

--

Putting these ideas together, here is our .def.orange[criterion to isolate causal effects]:
&gt; If we remove all edges flowing .b[out] of `\(\text{X}\)` (its .pink[causal effects]),
&lt;br&gt;then `\(\text{X}\)` and `\(\text{Y}\)` should be .purple[d-separated].
--

This criterion ensures that we've closed the .def.slate[backdoor paths] that generate non-causal associations between `\(\text{X}\)` and `\(\text{Y}\)`.

---
# Theory
.note[Building block 3:] .b.slate[Chains with conditions]

&lt;img src="SCM_files/figure-html/bb3-plot-6-1.svg" style="display: block; margin: auto;" /&gt;

.b.pink[Proof:] We want to show `\(\text{A}\)` and `\(\text{C}\)` are independent conditional on `\(\text{B}\)`,
&lt;br&gt;*i.e.*, `\(P(\text{A},\text{C}|\text{B})=P(\text{A}|\text{B})P(\text{C}|\text{B})\)`.

--

Start with BN factorization: `\(P(\text{A},\text{B},\text{C})\)`
--
 `\(= P(\text{A})P(\text{B}|\text{A})P(\text{C}|\text{B})\)`.

--

Now apply Bayes' rule for the LHS of our goal: `\(P(\text{A},\text{C}|\text{B}) = \frac{P(\text{A},\text{B},\text{C})}{P(\text{B})}\)`.

--

And substitute our factorization into the Bayes' rule expression:

`\(P(\text{A},\text{C}|\text{B}) = \dfrac{P(\text{A})P(\text{B}|\text{A})\color{#e64173}{P(\text{C}|\text{B})}}{P(\text{B})}\)`
--
 `\(=P(\text{A}|\text{B})\color{#e64173}{P(\text{C}|\text{B})}\)` `\(\checkmark\)` .grey-light[(Bayes rule again)]
 
 ---
.note[Building block 4:] .b.slate[Blocked forks]

&lt;img src="SCM_files/figure-html/bb4-plot-5-1.svg" style="display: block; margin: auto;" /&gt;

.b.pink[Proof:] We want to show `\(P(\text{A},\text{C}|\text{B})=P(\text{A}|\text{B})P(\text{C}|\text{B})\)`.

--

.note[Step 1:] Bayesian net. factorization: `\(P(\text{A},\text{B},\text{C})=P(\text{B})\color{#e64173}{P(\text{A}|\text{B})P(\text{C}|\text{B})}\)`

--

.note[Step 2:] Bayes' rule: `\(P(\text{A},\text{C}|\text{B})=\frac{P(\text{A},\text{B},\text{C})}{P(\text{B})}\)`

--

.note[Step 3:] Combine .note[2] &amp; .note[1]: `\(P(\text{A},\text{C}|\text{B})=\frac{P(\text{A},\text{B},\text{C})}{P(\text{B})} = \color{#e64173}{P(\text{A}|\text{B})P(\text{C}|\text{B})}\)` `\(\checkmark\)`


---

exclude: true


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
