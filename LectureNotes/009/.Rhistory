dml1wf = workflow() %>% add_recipe(dmlrecipe1) %>% add_model(tune_spec)
dml2wf = workflow() %>% add_recipe(dmlrecipe2) %>% add_model(tune_spec)
dat1folds = vfold_cv(dat1, 5)
dat2folds = vfold_cv(dat2, 5)
tune_res1 = tune_grid(
dml1wf,
resamples = dat1folds,
grid = 2
) %>% select_best(maximize = FALSE)
tune_res2 = tune_grid(
dml2wf,
resamples = dat2folds,
grid = 2
) %>% select_best(maximize = FALSE)
tune_best1 = dml1wf %>% finalize_workflow(tune_res1) %>% fit(data = dat1)
tune_best2 = dml2wf%>% finalize_workflow(tune_res1) %>% fit(data = dat2)
predict1 = predict(tune_best2, dat1)
predict2 = predict(tune_best1, dat2)
dat_out$inc_hat = rbind(predict1, predict2)
### STEP 3b: get the residuals Y - Y_hat on set 2 and on set 1
dat_out$inc_resid = rbind(dat1$incwage - predict1, dat2$incwage - predict2)
### STEP 4: regress (Y - Y_hat) on (X - X_hat) and return predictions
reg1 = lm(inc_resid ~ educ_resid, data= dat_out)
dat_out$ability = reg1$fitted.values
return(dat_out)
}
doubleml(datain)
p_load(magrittr)
doubleml <- function(data) {
### STEP 1: split X, W and Y into 2 random sets (done for you)
data %<>% mutate(educ = as.numeric(educ), sei = as.numeric(sei), hwsei = as.numeric(hwsei), presgl = as.numeric(presgl), prent = as.numeric(prent), incwage = as.numeric(incwage))
split <- initial_split(data, prop = .5)
dat1 = training(split)
dat2 = testing(split)
dat_out = rbind(dat1, dat2)
### STEP 2a: use a SuperLearner to train a model for E[X|W] on set 1 and predict X on set 2 using this model. Do the same but training on set 2 and predicting on set 1
dmlrecipe1 = recipe(educ ~ sei + hwsei + presgl + prent, data = dat1)
dmlrecipe2 = recipe(educ ~ sei + hwsei + presgl + prent, data = dat2)
tune_spec <- rand_forest(
mtry = 3,
trees = 1000,
min_n = 5
) %>%
set_mode("regression") %>%
set_engine("ranger")
dml1wf = workflow() %>% add_recipe(dmlrecipe1) %>% add_model(tune_spec)
dml2wf = workflow() %>% add_recipe(dmlrecipe2) %>% add_model(tune_spec)
dat1folds = vfold_cv(dat1, 5)
dat2folds = vfold_cv(dat2, 5)
tune_res1 = tune_grid(
dml1wf,
resamples = dat1folds,
grid = 2
) %>% select_best(maximize = FALSE)
tune_res2 = tune_grid(
dml2wf,
resamples = dat2folds,
grid = 2
) %>% select_best(maximize = FALSE)
tune_best1 = dml1wf %>% finalize_workflow(tune_res1) %>% fit(data = dat1)
tune_best2 = dml2wf%>% finalize_workflow(tune_res1) %>% fit(data = dat2)
final1 = finalize_model(
tune_spec,
tune_best1
) %>% last_fit(dat1folds)
final2 = finalize_model(
tune_spec,
tune_best2
) %>% last_fit(dat2folds)
predict1 = predict(tune_best2, dat1)
predict2 = predict(tune_best1, dat2)
dat_out$educ_hat = rbind(predict1, predict2)
### STEP 2b: get the residuals X - X_hat on set 2 and on set 1
dat_out$educ_resid = rbind(dat1$educ - predict1, dat2$educ - predict2)
### STEP 3a: use a SuperLearner to train a model for E[Y|W] on set 1 and predict Y on set 2 using this model. Do the same but training on set 2 and predicting on set 1
dmlrecipe1 = recipe(incwage ~ sei + hwsei + presgl + prent, data = dat1)
dmlrecipe2 = recipe(incwage ~ sei + hwsei + presgl + prent, data = dat2)
tune_spec <- rand_forest(
mtry = tune(),
trees = 100,
min_n = tune()
) %>%
set_mode("regression") %>%
set_engine("ranger")
dml1wf = workflow() %>% add_recipe(dmlrecipe1) %>% add_model(tune_spec)
dml2wf = workflow() %>% add_recipe(dmlrecipe2) %>% add_model(tune_spec)
dat1folds = vfold_cv(dat1, 5)
dat2folds = vfold_cv(dat2, 5)
tune_res1 = tune_grid(
dml1wf,
resamples = dat1folds,
grid = 2
) %>% select_best(maximize = FALSE)
tune_res2 = tune_grid(
dml2wf,
resamples = dat2folds,
grid = 2
) %>% select_best(maximize = FALSE)
tune_best1 = dml1wf %>% finalize_workflow(tune_res1) %>% fit(data = dat1)
tune_best2 = dml2wf%>% finalize_workflow(tune_res1) %>% fit(data = dat2)
predict1 = predict(tune_best2, dat1)
predict2 = predict(tune_best1, dat2)
dat_out$inc_hat = rbind(predict1, predict2)
### STEP 3b: get the residuals Y - Y_hat on set 2 and on set 1
dat_out$inc_resid = rbind(dat1$incwage - predict1, dat2$incwage - predict2)
### STEP 4: regress (Y - Y_hat) on (X - X_hat) and return predictions
reg1 = lm(inc_resid ~ educ_resid, data= dat_out)
dat_out$ability = reg1$fitted.values
return(dat_out)
}
doubleml(datain)
setwd('~/Desktop/GithubProjects/Econometrics/EC421/Spring2021/Project')
ddi <- read_ipums_ddi("usa_00010.xml")
data <- read_ipums_micro(ddi)
#softmax function to find percentage weighting scheme with no distributional assumptions
data %<>% mutate(perwt_nrm = (PERWT - min(PERWT))/max(PERWT),perwt_sftmx= exp(perwt_nrm)/sum(exp(perwt_nrm))) %>% filter(AGE > 23)
# Load data ------------------------------------------------------------------------------
# Load ACS data
acs_raw = data
names(acs_raw) = tolower(names(acs_raw))
# Grab variable labels
var_desc = lapply(acs_raw, function(x) attributes(x)$label)
# Load ERS data
ers_raw = here("ers-rural-urban-2013.xls") %>% read_xls(sheet = 1)
# Data work: Prep ERS data ---------------------------------------------------------------
# Grab and name desired columns
ers_clean = transmute(
ers_raw,
fips = FIPS,
i_urban = 1 * (RUCC_2013 <= 5)
)
setDT(ers_clean)
# Data work: upsample rural weights
setDT(acs_raw)
acs_raw[, `:=`(fips = paste0(str_pad(statefip, 2, "left", 0), str_pad(countyfip, 3, "left", 0)))]
acs_raw = merge(
x = ers_clean,
y = acs_raw,
by = "fips",
all.x = F,
all.y = T
)
# Fix missingness from small areas (assuming rural)
acs_raw[is.na(i_urban), i_urban := .5]
acs_raw[, `:=`(perwt_sftmx = perwt_sftmx+(i_urban*-sd(acs_raw$perwt_sftmx)))]
acs_raw[i_urban == .5, i_urban := 0]
p_load(tidymodels, tidyverse, readxl, haven, data.table, broom, magrittr, here, ipumsr)
setwd('~/Desktop/GithubProjects/Econometrics/EC421/Spring2021/Project')
ddi <- read_ipums_ddi("usa_00010.xml")
data <- read_ipums_micro(ddi)
#softmax function to find percentage weighting scheme with no distributional assumptions
data %<>% mutate(perwt_nrm = (PERWT - min(PERWT))/max(PERWT),perwt_sftmx= exp(perwt_nrm)/sum(exp(perwt_nrm))) %>% filter(AGE > 23)
# Load data ------------------------------------------------------------------------------
# Load ACS data
acs_raw = data
names(acs_raw) = tolower(names(acs_raw))
# Grab variable labels
var_desc = lapply(acs_raw, function(x) attributes(x)$label)
# Load ERS data
ers_raw = here("ers-rural-urban-2013.xls") %>% read_xls(sheet = 1)
# Data work: Prep ERS data ---------------------------------------------------------------
# Grab and name desired columns
ers_clean = transmute(
ers_raw,
fips = FIPS,
i_urban = 1 * (RUCC_2013 <= 5)
)
setDT(ers_clean)
# Data work: upsample rural weights
setDT(acs_raw)
acs_raw[, `:=`(fips = paste0(str_pad(statefip, 2, "left", 0), str_pad(countyfip, 3, "left", 0)))]
acs_raw = merge(
x = ers_clean,
y = acs_raw,
by = "fips",
all.x = F,
all.y = T
)
# Fix missingness from small areas (assuming rural)
acs_raw[is.na(i_urban), i_urban := .5]
acs_raw[, `:=`(perwt_sftmx = perwt_sftmx+(i_urban*-sd(acs_raw$perwt_sftmx)))]
acs_raw[i_urban == .5, i_urban := 0]
datain = acs_raw %>% sample_n((2000))
doubleml(datain)
doubleml <- function(data) {
### STEP 1: split X, W and Y into 2 random sets (done for you)
data %<>% mutate(educ = as.numeric(educ), sei = as.numeric(sei), hwsei = as.numeric(hwsei), presgl = as.numeric(presgl), prent = as.numeric(prent), incwage = as.numeric(incwage))
split <- initial_split(data, prop = .5)
dat1 = training(split)
dat2 = testing(split)
dat_out = rbind(dat1, dat2)
### STEP 2a: use a SuperLearner to train a model for E[X|W] on set 1 and predict X on set 2 using this model. Do the same but training on set 2 and predicting on set 1
dmlrecipe1 = recipe(educ ~ sei + hwsei + presgl + prent, data = dat1)
dmlrecipe2 = recipe(educ ~ sei + hwsei + presgl + prent, data = dat2)
tune_spec <- rand_forest(
mtry = 3,
trees = 1000,
min_n = tune()
) %>%
set_mode("regression") %>%
set_engine("ranger")
dml1wf = workflow() %>% add_recipe(dmlrecipe1) %>% add_model(tune_spec)
dml2wf = workflow() %>% add_recipe(dmlrecipe2) %>% add_model(tune_spec)
dat1folds = vfold_cv(dat1, 5)
dat2folds = vfold_cv(dat2, 5)
tune_res1 = tune_grid(
dml1wf,
resamples = dat1folds,
grid = 2
) %>% select_best(maximize = FALSE)
tune_res2 = tune_grid(
dml2wf,
resamples = dat2folds,
grid = 2
) %>% select_best(maximize = FALSE)
tune_best1 = dml1wf %>% finalize_workflow(tune_res1) %>% fit(data = dat1)
tune_best2 = dml2wf%>% finalize_workflow(tune_res1) %>% fit(data = dat2)
final1 = finalize_model(
tune_spec,
tune_best1
) %>% last_fit(dat1folds)
final2 = finalize_model(
tune_spec,
tune_best2
) %>% last_fit(dat2folds)
predict1 = predict(tune_best2, dat1)
predict2 = predict(tune_best1, dat2)
dat_out$educ_hat = rbind(predict1, predict2)
### STEP 2b: get the residuals X - X_hat on set 2 and on set 1
dat_out$educ_resid = rbind(dat1$educ - predict1, dat2$educ - predict2)
### STEP 3a: use a SuperLearner to train a model for E[Y|W] on set 1 and predict Y on set 2 using this model. Do the same but training on set 2 and predicting on set 1
dmlrecipe1 = recipe(incwage ~ sei + hwsei + presgl + prent, data = dat1)
dmlrecipe2 = recipe(incwage ~ sei + hwsei + presgl + prent, data = dat2)
tune_spec <- rand_forest(
mtry = 3,
trees = 100,
min_n = tune()
) %>%
set_mode("regression") %>%
set_engine("ranger")
dml1wf = workflow() %>% add_recipe(dmlrecipe1) %>% add_model(tune_spec)
dml2wf = workflow() %>% add_recipe(dmlrecipe2) %>% add_model(tune_spec)
dat1folds = vfold_cv(dat1, 5)
dat2folds = vfold_cv(dat2, 5)
tune_res1 = tune_grid(
dml1wf,
resamples = dat1folds,
grid = 2
) %>% select_best(maximize = FALSE)
tune_res2 = tune_grid(
dml2wf,
resamples = dat2folds,
grid = 2
) %>% select_best(maximize = FALSE)
tune_best1 = dml1wf %>% finalize_workflow(tune_res1) %>% fit(data = dat1)
tune_best2 = dml2wf%>% finalize_workflow(tune_res1) %>% fit(data = dat2)
predict1 = predict(tune_best2, dat1)
predict2 = predict(tune_best1, dat2)
dat_out$inc_hat = rbind(predict1, predict2)
### STEP 3b: get the residuals Y - Y_hat on set 2 and on set 1
dat_out$inc_resid = rbind(dat1$incwage - predict1, dat2$incwage - predict2)
### STEP 4: regress (Y - Y_hat) on (X - X_hat) and return predictions
reg1 = lm(inc_resid ~ educ_resid, data= dat_out)
dat_out$ability = reg1$fitted.values
return(dat_out)
}
doubleml(datain)
datain %<>% mutate(educ = as.numeric(educ), sei = as.numeric(sei), hwsei = as.numeric(hwsei), presgl = as.numeric(presgl), prent = as.numeric(prent), incwage = as.numeric(incwage))
split <- initial_split(data, prop = .5)
dat1 = training(datain)
dat2 = testing(datain)
dat_out = rbind(dat1, dat2)
### STEP 2a: use a SuperLearner to train a model for E[X|W] on set 1 and predict X on set 2 using this model. Do the same but training on set 2 and predicting on set 1
dmlrecipe1 = recipe(educ ~ sei + hwsei + presgl + prent, data = dat1)
dmlrecipe2 = recipe(educ ~ sei + hwsei + presgl + prent, data = dat2)
tune_spec <- rand_forest(
mtry = 3,
trees = 1000,
min_n = tune()
) %>%
set_mode("regression") %>%
set_engine("ranger")
dml1wf = workflow() %>% add_recipe(dmlrecipe1) %>% add_model(tune_spec)
dml2wf = workflow() %>% add_recipe(dmlrecipe2) %>% add_model(tune_spec)
dat1folds = vfold_cv(dat1, 5)
dat2folds = vfold_cv(dat2, 5)
tune_res1 = tune_grid(
dml1wf,
resamples = dat1folds,
grid = 2
) %>% select_best(maximize = FALSE)
tune_res2 = tune_grid(
dml2wf,
resamples = dat2folds,
grid = 2
) %>% select_best(maximize = FALSE)
datain %<>% mutate(educ = as.numeric(educ), sei = as.numeric(sei), hwsei = as.numeric(hwsei), presgl = as.numeric(presgl), prent = as.numeric(prent), incwage = as.numeric(incwage))
split <- initial_split(datain, prop = .5)
dat1 = training(datain)
dat2 = testing(datain)
dat_out = rbind(dat1, dat2)
### STEP 2a: use a SuperLearner to train a model for E[X|W] on set 1 and predict X on set 2 using this model. Do the same but training on set 2 and predicting on set 1
dmlrecipe1 = recipe(educ ~ sei + hwsei + presgl + prent, data = dat1)
dmlrecipe2 = recipe(educ ~ sei + hwsei + presgl + prent, data = dat2)
tune_spec <- rand_forest(
mtry = 3,
trees = 1000,
min_n = tune()
) %>%
set_mode("regression") %>%
set_engine("ranger")
dml1wf = workflow() %>% add_recipe(dmlrecipe1) %>% add_model(tune_spec)
dml2wf = workflow() %>% add_recipe(dmlrecipe2) %>% add_model(tune_spec)
dat1folds = vfold_cv(dat1, 5)
dat2folds = vfold_cv(dat2, 5)
tune_res1 = tune_grid(
dml1wf,
resamples = dat1folds,
grid = 2
) %>% select_best(maximize = FALSE)
tune_res2 = tune_grid(
dml2wf,
resamples = dat2folds,
grid = 2
) %>% select_best(maximize = FALSE)
datain %<>% mutate(educ = as.numeric(educ), sei = as.numeric(sei), hwsei = as.numeric(hwsei), presgl = as.numeric(presgl), prent = as.numeric(prent), incwage = as.numeric(incwage))
split <- initial_split(datain, prop = .5)
dat1 = training(split)
dat2 = testing(split)
dat_out = rbind(dat1, dat2)
### STEP 2a: use a SuperLearner to train a model for E[X|W] on set 1 and predict X on set 2 using this model. Do the same but training on set 2 and predicting on set 1
dmlrecipe1 = recipe(educ ~ sei + hwsei + presgl + prent, data = dat1)
dmlrecipe2 = recipe(educ ~ sei + hwsei + presgl + prent, data = dat2)
tune_spec <- rand_forest(
mtry = 3,
trees = 1000,
min_n = tune()
) %>%
set_mode("regression") %>%
set_engine("ranger")
dml1wf = workflow() %>% add_recipe(dmlrecipe1) %>% add_model(tune_spec)
dml2wf = workflow() %>% add_recipe(dmlrecipe2) %>% add_model(tune_spec)
dat1folds = vfold_cv(dat1, 5)
dat2folds = vfold_cv(dat2, 5)
tune_res1 = tune_grid(
dml1wf,
resamples = dat1folds,
grid = 2
) %>% select_best(maximize = FALSE)
tune_res2 = tune_grid(
dml2wf,
resamples = dat2folds,
grid = 2
) %>% select_best(maximize = FALSE)
tune_best1 = dml1wf %>% finalize_workflow(tune_res1) %>% fit(data = dat1)
tune_best2 = dml2wf%>% finalize_workflow(tune_res1) %>% fit(data = dat2)
final1 = finalize_model(
tune_spec,
tune_best1
)
final2 = finalize_model(
tune_spec,
tune_best2
)
final1 = finalize_model(
tune_spec,
tune_best1
) %>% last_fit(dat1folds)
final2 = finalize_model(
tune_spec,
tune_best2
) %>% last_fit(dat2folds)
fit_resamples(final2)
tune_best1 %>% fit_resamples()
tune_best1 %>% fit_resamples(dat1)
tune_best1 %>% fit_resamples(dat1folds)
tune_best1 = dml1wf %>% finalize_workflow(tune_res1) %>% fit_resamples(data = dat1)
tune_best2 = dml2wf%>% finalize_workflow(tune_res1) %>% fit_resamples(data = dat2)
predict1 = predict(final2, dat1)
predict2 = predict(final1, dat2)
tune_best1 = dml1wf %>% finalize_workflow(tune_res1) %>% fit_resamples(data = dat1folds)
tune_best2 = dml2wf%>% finalize_workflow(tune_res1) %>% fit_resamples(data = dat2folds)
predict1 = predict(final2, dat1)
predict2 = predict(final1, dat2)
tune_best1 = dml1wf %>% finalize_workflow(tune_res1) %>% fit_resamples(dat1folds)
tune_best2 = dml2wf%>% finalize_workflow(tune_res1) %>% fit_resamples(dat2folds)
predict1 = predict(tune_best2, dat1)
predict2 = predict(tune_best1, dat2)
dat1 %>% data.table()
predict1 = predict(tune_best2, dat1 %>% data.table())
predict2 = predict(tune_best1, dat2 %>% data.table())
tune_best2
tune_best2 %>% predict()
p_load(workflows)
tune_best1 = dml1wf %>% finalize_workflow(tune_res1) %>% fit(dat1)
tune_best2 = dml2wf%>% finalize_workflow(tune_res1) %>% fit(dat2)
predict1 = predict(tune_best2, dat1 %>% data.table())
predict2 = predict(tune_best1, dat2 %>% data.table())
predict1
rbind(predict1, predict2)
rbind(dat1$educ - predict1, dat2$educ - predict2)
dmlrecipe1 = recipe(incwage ~ sei + hwsei + presgl + prent, data = dat1)
dmlrecipe2 = recipe(incwage ~ sei + hwsei + presgl + prent, data = dat2)
tune_spec <- rand_forest(
mtry = 3,
trees = 100,
min_n = tune()
) %>%
set_mode("regression") %>%
set_engine("ranger")
dml1wf = workflow() %>% add_recipe(dmlrecipe1) %>% add_model(tune_spec)
dml2wf = workflow() %>% add_recipe(dmlrecipe2) %>% add_model(tune_spec)
dat1folds = vfold_cv(dat1, 5)
dat2folds = vfold_cv(dat2, 5)
tune_res1 = tune_grid(
dml1wf,
resamples = dat1folds,
grid = 2
) %>% select_best(maximize = FALSE)
tune_res2 = tune_grid(
dml2wf,
resamples = dat2folds,
grid = 2
) %>% select_best(maximize = FALSE)
tune_best1 = dml1wf %>% finalize_workflow(tune_res1) %>% fit(dat1)
tune_best2 = dml2wf%>% finalize_workflow(tune_res1) %>% fit(dat2)
predict1 = predict(tune_best2, dat1)
predict2 = predict(tune_best1, dat2)
pagedown::chrome_print(
input = "09-nonstationarity.html",
timeout = 60
)
datin = fread('/Users/connor/Dropbox (University of Oregon)/WUI-AFRI/data/duplicates_and_toy_data/duplicates_and_toy_data/inap_duplicated_coords.csv')
p_load(data.table)
library(pacman)
p_load(data.table, tidyverse)
datin = fread('/Users/connor/Dropbox (University of Oregon)/WUI-AFRI/data/duplicates_and_toy_data/duplicates_and_toy_data/inap_duplicated_coords.csv')
p_load(haven)
read_dta('/Users/connor/Dropbox (University of Oregon)/WUI-AFRI/data/duplicates_and_toy_data/duplicates_and_toy_data/ztrax_hmda_data_CA.dta')
datin = read_dta('/Users/connor/Dropbox (University of Oregon)/WUI-AFRI/data/duplicates_and_toy_data/duplicates_and_toy_data/ztrax_hmda_data_CA.dta')
datin$census_tract
library(pacman)
p_load(gmodels,
raster,
tidyverse,
gtools,
RPostgres,
sqldf,
RODBC,
DT,
rgdal,
UScensus2000tract,
sf,
sp,
rgeos,
spatialEco,
tigris,
dbplyr,
data.table,
here,
stringi,
elevatr,
haven,
rgdal,
gdalUtils)
# Dataload & Pruning
#-----------------------------------------
zillowdbloc <- '/Volumes/G-DRIVE-mobile-SSD-R-Series/ZTrans/sorting_wui.sqlite'
ztrax_database <- dbConnect(RSQLite::SQLite(), zillowdbloc)
hmda_database <- dbConnect(Postgres(), host = 'localhost', dbname = 'hmda',
password = '', user = 'postgres')
hmda_data = tbl(hmda_database,  in_schema('hmda_public','lar_2015'))
hmda_data %>% select(county_code)
hmda_data = tbl(hmda_database,  in_schema('hmda_public','lar_2015'))
hmda_data
hmda_code = tbl(hmda_database,  in_schema('hmda_public','lar_2015')) %>% select(county_code)
hmda_code = hmda_data %>% select(county_code)
p_load(tidyverse)
hmda_code = hmda_data %>% select(county_code)
p_load(dbplyr)
hmda_code = hmda_data %>% select(county_code)
hmda_data = tbl(hmda_database,  in_schema('hmda_public',tablename))
hmda_data = tbl(hmda_database,  in_schema('hmda_public','lar_2015'))
hmda_data$county_code
hmda_data
datin$respondent_id_prop
datin$RecordingDate
datin %>% mutate(RecordingDate = as.Date(RecordingDate)) %>% arrange(RecordingDate)
datin %>% mutate(RecordingDate = as.Date(RecordingDate)) %>% arrange(RecordingDate) %>% select(respondent_id_prop) %>% unique()
datin %>% mutate(RecordingDate = as.Date(RecordingDate)) %>% arrange(RecordingDate) %>% dplyr::select(respondent_id_prop) %>% unique()
out_dat = readRDS('/Users/connor/Desktop/GithubProjects/propval-wui/sorting-wui/intermediate/final_hmdazil.dta') %>%
select(ImportParcelID, TransId, respondent_id_prop, sequence_num,
InitialInterestRate, RecordingDate, loan_amount, SalesPriceAmount,
PropertyLandUseStndCode, year)
out_dat = readRDS('/Users/connor/Desktop/GithubProjects/propval-wui/sorting-wui/intermediate/final_hmdazil.dta') %>%
dplyr::select(ImportParcelID, TransId, respondent_id_prop, sequence_num,
InitialInterestRate, RecordingDate, loan_amount, SalesPriceAmount,
PropertyLandUseStndCode, year)
out_dat = readRDS('/Users/connor/Desktop/GithubProjects/propval-wui/sorting-wui/intermediate/final_hmdazil.dta') %>%
dplyr::select(ImportParcelID, TransId, respondent_id_prop, sequence_num,
InitialInterestRate, RecordingDate, loan_amount, SalesPriceAmount,
PropertyLandUseStndCode, year) %>% filter(year < 2016 & year > 2005)
out_dat
out_dat = readRDS('/Users/connor/Desktop/GithubProjects/propval-wui/sorting-wui/intermediate/final_hmdazil.dta') %>%
select(ImportParcelID, TransId, respondent_id_prop, sequence_num,
InitialInterestRate, RecordingDate, loan_amount, SalesPriceAmount,
PropertyLandUseStndCode, year)
out_dat = readRDS('/Users/connor/Desktop/GithubProjects/propval-wui/sorting-wui/intermediate/final_hmdazil.dta') %>%
dplyr::select(ImportParcelID, TransId, respondent_id_prop, sequence_num,
InitialInterestRate, RecordingDate, loan_amount, SalesPriceAmount,
PropertyLandUseStndCode, year)
out_dat$respondent_id_prop
